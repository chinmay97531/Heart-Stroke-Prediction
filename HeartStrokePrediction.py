# -*- coding: utf-8 -*-
"""HeartStroke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hRo09dAjpxd0mT0tbH9CYT9PKTCdWtM8
"""

import numpy as np
import pandas as pd
import os

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import FunctionTransformer
from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from sklearn.neighbors import KNeighborsClassifier
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Data/healthcare-dataset-stroke-data.csv'

data = pd.read_csv(file_path)

print(data.dtypes)

data.stroke.value_counts()

data_copy = data.copy(deep=True)

print(data_copy.sample(3))

sns.countplot(data = data, x = 'stroke')
plt.title('Number of Patients')

data.isnull()

data.isnull().any()

print(data.isnull().sum())
print(data.isnull().sum()/len(data)*100)

data_copy = data_copy.drop(['id'], axis=1)
data_copy

print(data_copy.duplicated().sum())
data_copy.isnull().sum()

data_copy["bmi"] = data_copy["bmi"].fillna(data_copy["bmi"].mean())
print(data_copy.isnull().sum())

cat_features = [feature for feature in data_copy.columns if data[feature].dtypes == 'O']
num_features = [feature for feature in data_copy.columns if data[feature].dtypes != 'O']

print(cat_features)
print(num_features)

for col in cat_features[:]:
  plt.figure(figsize = (9, 3), dpi=100)
  sns.countplot(data = data, x = col, hue = 'stroke', palette='gist_rainbow_r')
  plt.title(col)

for col in num_features :
  plt.figure(figsize = (9, 3), dpi=100)
  sns.barplot(data = data_copy, x = 'stroke', y = col, hue='stroke', palette='gist_rainbow_r')
  plt.title(col)

cat_features = data_copy.select_dtypes(include='object')
num_features = data_copy.select_dtypes(include='number')
cat_features.head()
num_features.head()

corr = num_features.corr()
plt.figure(figsize=(8,8))
sns.heatmap(corr, annot=True, cmap='Spectral').set(title='Correlation Matrix')

cat_features_encoded = pd.get_dummies(cat_features, columns=cat_features.columns.to_list(), dtype=int)
cat_features_encoded.head()

data_copy = pd.concat([cat_features_encoded, num_features], axis=1, join = 'outer')
print(data_copy.head())
print(data_copy.ndim)
data_copy.columns

corr = data_copy.corr()
plt.figure(figsize=(20,20))
sns.heatmap(corr, annot=True, cmap='Spectral').set(title='Correlation Matrix')

y = data_copy['stroke']
x = data_copy.drop(['stroke'], axis=1)
print(x.shape)
print(y.shape)

sc = StandardScaler()
x = sc.fit_transform(x)
x

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=7)
X_train.shape, X_test.shape

accuracy = {}

"""# **Logistic Regression**"""

lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)
y_pred1 = lr.predict(X_test)
print(accuracy_score(y_test, y_pred1))
accuracy[str(lr)] = accuracy_score(y_test, y_pred1)*100

cm = confusion_matrix(y_test, y_pred1)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(x, y)

X_resampled.shape, y_resampled.shape
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=7)
X_train.shape, X_test.shape

lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)
y_pred1 = lr.predict(X_test)
print(accuracy_score(y_test, y_pred1))
accuracy[str(lr)] = accuracy_score(y_test, y_pred1)*100

cm = confusion_matrix(y_test, y_pred1)
conf_matrix = pd.DataFrame(data=cm, columns=['Predicted:0', 'Predicted:1'], index=['Actual:0', 'Actual:1'])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""# **Random Forest**"""

rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)
rfc_train = rfc.predict(X_test)

print("Training Accuracy =", format(metrics.accuracy_score(y_test, rfc_train)))
accuracy[str(rfc)] = accuracy_score(y_test, y_pred1)*100

cm = confusion_matrix(y_test, rfc_train)
conf_matrix = pd.DataFrame(data=cm, columns=['Predicted:0', 'Predicted:1'], index=['Actual:0', 'Actual:1'])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""# **SVM**"""

svc_model = SVC()
svc_model.fit(X_train, y_train)
svc_pred = svc_model.predict(X_test)
print("Test Accuracy =", format(metrics.accuracy_score(y_test, svc_pred)))
accuracy[str(svc_model)] = accuracy_score(y_test, y_pred1)*100

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, svc_pred))

cm = confusion_matrix(y_test, svc_pred)
conf_matrix = pd.DataFrame(data=cm, columns=['Predicted:0', 'Predicted:1'], index=['Actual:0', 'Actual:1'])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()